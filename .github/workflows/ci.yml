name: Scrape Georgia Legislation

# Trigger configuration
on:
  # Manual trigger - allows running the workflow from GitHub UI
  # Users can optionally specify max_pages parameter to limit scraping
  workflow_dispatch:
    inputs:
      max_pages:
        description: Maximum pages to scrape (leave empty for all pages)
        required: false
        default: ''
  # Scheduled trigger
  schedule:
  - cron: 0 4 * * *

jobs:
  scrape:
    runs-on: ubuntu-latest
    # 2-hour timeout for full session scrapes
    # Adjust if your scraping takes longer
    timeout-minutes: 120

    steps:
    # Step 1: Clone the repository to get the latest code
    - name: Checkout repository
      uses: actions/checkout@v6

    # Step 2: Set up Python 3.11 environment
    # Matches local development requirements
    - name: Set up Python
      uses: actions/setup-python@v6
      with:
        python-version: '3.11'

    # Step 3: Install Python dependencies and Playwright browser
    # - requests: HTTP library for making web requests
    # - beautifulsoup4: HTML parsing library
    # - playwright: Browser automation for JavaScript rendering
    # - aiohttp: Async HTTP client for concurrent requests
    # - chromium: Headless browser for rendering JavaScript-heavy pages
    - name: Install dependencies
      run: |
        pip install requests beautifulsoup4 playwright aiohttp
        playwright install chromium

    # Step 4: Set up Node.js for validation scripts
    - name: Set up Node.js
      uses: actions/setup-node@v6
      with:
        node-version: '20'

    # Step 5: Install Node.js validation tools
    - name: Install Node.js tools
      run: npm install

    # Step 6: Run the scraper
    # - Conditionally passes max_pages parameter if provided
    #   (useful if website is temporarily down)
    # Step 6: Run the scraper
    # - Respectfully scrapes with configurable delays to respect server
    # - Uses environment variables to control request rate
    # - continue-on-error: true allows workflow to continue if server blocks IPs
    - name: Run scraper
      continue-on-error: true
      env:
        SCRAPER_DELAY: '2'
        SCRAPER_CONCURRENCY: '2'
      run: |
        if [ -n "${{ github.event.inputs.max_pages }}" ]; then
          echo "Scraping maximum ${{ github.event.inputs.max_pages }} pages..."
          echo "Using respectful settings: 2s delay, 2 concurrent requests"
          python backend/scraper.py ${{ github.event.inputs.max_pages }}
        else
          echo "Scraping all available pages..."
          echo "Using respectful settings: 2s delay, 2 concurrent requests"
          python backend/scraper.py
        fi

    # Step 7: Verify that output file was created
    # - Sets output variable 'exists' to true/false
    # - Used by subsequent steps to conditionally run
    - name: Check if output file exists
      id: check_file
      run: |
        if [ -f ga_legislation.json ]; then
          echo "exists=true" >> $GITHUB_OUTPUT
          echo "JSON file created successfully"
          echo "File size: $(wc -c < ga_legislation.json) bytes"
        else
          echo "exists=false" >> $GITHUB_OUTPUT
          echo "Warning: JSON file was not created"
        fi

    # Step 8: Validate JSON schema
    # - Validates structure and content against defined schema
    # - Checks required fields, data types, and formats
    # - Reports statistics on valid/invalid bills
    - name: Validate JSON schema
      if: steps.check_file.outputs.exists == 'true'
      run: node scripts/validate-schema.js ga_legislation.json

    # Step 9: Generate timestamp for artifact naming
    # - Used to create unique artifact names for each run
    # - Helps track when data was collected
    - name: Get current date
      id: date
      run: echo "date=$(date +'%Y-%m-%d_%H-%M-%S')" >> $GITHUB_OUTPUT

    # Step 10: Upload results as GitHub Artifact
    # - Only runs if output file was successfully created
    # - Artifacts are retained for 90 days
    # - Can be downloaded from the Actions tab in GitHub
    - name: Upload JSON as artifact
      if: steps.check_file.outputs.exists == 'true'
      uses: actions/upload-artifact@v6
      with:
        name: ga-legislation-${{ steps.date.outputs.date }}
        path: ga_legislation.json
        retention-days: 7

    # Step 11: Commit and push results to repository
    # Automatically commits scraped data to the repository
    # Uses semantic versioning for commit messages
    # Note: Using -f flag to force-add ga_legislation.json even though it's in .gitignore
    # This allows CI to commit generated data while keeping local copies ignored
    # Only runs on scheduled runs (daily) which execute on main branch
    - name: Commit and push if changes
      if: |
        steps.check_file.outputs.exists == 'true' &&
        github.event_name == 'schedule'
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        git add -f ga_legislation.json
        git diff --quiet && git diff --staged --quiet || (
          git commit -m "chore: update legislation data - ${{ steps.date.outputs.date }}"
          git push origin main
        )
