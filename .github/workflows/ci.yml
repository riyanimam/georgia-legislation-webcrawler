name: Scrape Georgia Legislation

# Trigger configuration
on:
  # Manual trigger - allows running the workflow from GitHub UI
  # Users can optionally specify max_pages parameter to limit scraping
  workflow_dispatch:
    inputs:
      max_pages:
        description: Maximum pages to scrape (leave empty for all pages)
        required: false
        default: ''
  # Scheduled trigger
  schedule:
  - cron: 0 4 * * *

jobs:
  scrape:
    runs-on: ubuntu-latest
    # 2-hour timeout for full session scrapes
    # Adjust if your scraping takes longer
    timeout-minutes: 120

    steps:
    # Step 1: Clone the repository to get the latest code
    - name: Checkout repository
      uses: actions/checkout@v6

    # Step 2: Set up Python 3.11 environment
    # Matches local development requirements
    - name: Set up Python
      uses: actions/setup-python@v6
      with:
        python-version: '3.11'

    # Step 3: Install Python dependencies and Playwright browser
    # - requests: HTTP library for making web requests
    # - beautifulsoup4: HTML parsing library
    # - playwright: Browser automation for JavaScript rendering
    # - chromium: Headless browser for rendering JavaScript-heavy pages
    - name: Install dependencies
      run: |
        pip install requests beautifulsoup4 playwright
        playwright install chromium

    # Step 4: Run the scraper
    # - Conditionally passes max_pages parameter if provided
    # - continue-on-error: true allows workflow to continue even if scraper fails
    #   (useful if website is temporarily down)
    - name: Run scraper
      continue-on-error: true
      run: |
        if [ -n "${{ github.event.inputs.max_pages }}" ]; then
          echo "Scraping maximum ${{ github.event.inputs.max_pages }} pages..."
          python backend/scraper.py ${{ github.event.inputs.max_pages }}
        else
          echo "Scraping all available pages..."
          python backend/scraper.py
        fi

    # Step 5: Verify that output file was created
    # - Sets output variable 'exists' to true/false
    # - Used by subsequent steps to conditionally run
    - name: Check if output file exists
      id: check_file
      run: |
        if [ -f ga_legislation.json ]; then
          echo "exists=true" >> $GITHUB_OUTPUT
          echo "JSON file created successfully"
          echo "File size: $(wc -c < ga_legislation.json) bytes"
        else
          echo "exists=false" >> $GITHUB_OUTPUT
          echo "Warning: JSON file was not created"
        fi

    # Step 6: Generate timestamp for artifact naming
    # - Used to create unique artifact names for each run
    # - Helps track when data was collected
    - name: Get current date
      id: date
      run: echo "date=$(date +'%Y-%m-%d_%H-%M-%S')" >> $GITHUB_OUTPUT

    # Step 7: Upload results as GitHub Artifact
    # - Only runs if output file was successfully created
    # - Artifacts are retained for 90 days
    # - Can be downloaded from the Actions tab in GitHub
    - name: Upload JSON as artifact
      if: steps.check_file.outputs.exists == 'true'
      uses: actions/upload-artifact@v6
      with:
        name: ga-legislation-${{ steps.date.outputs.date }}
        path: ga_legislation.json
        retention-days: 7

    # Step 8: Optional - Commit and push results to repository
    # Uncomment below to automatically commit scraped data to the repository
    # This is disabled by default to avoid cluttering commit history
    # Enable if you want automatic data updates in your repo
    # - name: Commit and push if changes
    #   if: steps.check_file.outputs.exists == 'true'
    #   run: |
    #     git config --local user.email "github-actions[bot]@users.noreply.github.com"
    #     git config --local user.name "github-actions[bot]"
    #     git add ga_legislation.json
    #     git diff --quiet && git diff --staged --quiet || \
    #       (git commit -m "Update legislation data - ${{ steps.date.outputs.date }}" && git push)
