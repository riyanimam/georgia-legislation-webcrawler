name: Scrape Georgia Legislation

# Trigger configuration
on:
  # Manual trigger - allows running the workflow from GitHub UI
  # Users can optionally specify max_pages parameter to limit scraping
  workflow_dispatch:
    inputs:
      max_pages:
        description: Maximum pages to scrape (leave empty for all pages)
        required: false
        default: ''
  # Scheduled trigger (commented out by default)
  # Uncomment below to run automatically on a schedule
  # Example: run daily at 2 AM UTC
  # schedule:
  #   - cron: '0 2 * * *'

jobs:
  validate:
    # Code quality and format validation job
    # Mirrors pre-commit hooks to ensure code quality in CI
    runs-on: ubuntu-latest

    steps:
    # Step 1: Clone the repository
    - name: Checkout repository
      uses: actions/checkout@v6

    # Step 2: Set up Python 3.11 environment
    - name: Set up Python
      uses: actions/setup-python@v6
      with:
        python-version: '3.11'

    # Step 3: Install linting and formatting tools
    # - ruff: Fast Python linter and formatter
    # - mdformat: Markdown formatter with GFM support
    # - markdownlint-cli: Markdown linter
    - name: Install validation tools
      run: |
        pip install ruff mdformat mdformat-gfm mdformat-tables markdownlint-cli

    # Step 4: Run ruff linting
    # - Validates Python code against style and error rules
    # - Uses --exit-zero to report issues without blocking
    - name: Ruff lint
      run: ruff check scraper.py --fix --exit-zero

    # Step 5: Run ruff formatting check
    # - Ensures Python code matches 100-character line limit
    - name: Ruff format check
      run: ruff format scraper.py --line-length=100 --check

    # Step 6: Run markdown formatting
    # - Validates markdown syntax and consistency
    # - Uses GFM and table support extensions
    - name: Markdown format check
      run: mdformat README.md --wrap=100 --number --check

    # Step 7: Run markdownlint validation
    # - Checks markdown files for linting issues
    # - Disabled rules: MD013 (line length), MD024 (duplicate headers)
    - name: Markdownlint check
      run: markdownlint README.md --disable=MD013 --disable=MD024

    # Step 8: Install YAML validation tools
    # - yamllint: YAML linting tool
    - name: Install YAML validation tools
      run: pip install yamllint

    # Step 9: Run yamllint validation
    # - Validates YAML syntax and formatting in GitHub Actions workflow
    # - Uses strict mode to catch all issues
    - name: Yamllint check
      run: yamllint .github/workflows/ci.yml --format=parsable --strict

    # Step 10: Validate YAML formatting in pre-commit config
    - name: Yamllint pre-commit config
      run: yamllint .pre-commit-config.yaml --format=parsable --strict

  scrape:
    needs: validate
    runs-on: ubuntu-latest
    # 2-hour timeout for full session scrapes
    # Adjust if your scraping takes longer
    timeout-minutes: 120

    steps:
    # Step 1: Clone the repository to get the latest code
    - name: Checkout repository
      uses: actions/checkout@v6

    # Step 2: Set up Python 3.11 environment
    # Matches local development requirements
    - name: Set up Python
      uses: actions/setup-python@v6
      with:
        python-version: '3.11'

    # Step 3: Install Python dependencies and Playwright browser
    # - requests: HTTP library for making web requests
    # - beautifulsoup4: HTML parsing library
    # - playwright: Browser automation for JavaScript rendering
    # - chromium: Headless browser for rendering JavaScript-heavy pages
    - name: Install dependencies
      run: |
        pip install requests beautifulsoup4 playwright
        playwright install chromium

    # Step 4: Run the scraper
    # - Conditionally passes max_pages parameter if provided
    # - continue-on-error: true allows workflow to continue even if scraper fails
    #   (useful if website is temporarily down)
    - name: Run scraper
      continue-on-error: true
      run: |
        if [ -n "${{ github.event.inputs.max_pages }}" ]; then
          echo "Scraping maximum ${{ github.event.inputs.max_pages }} pages..."
          python scraper.py ${{ github.event.inputs.max_pages }}
        else
          echo "Scraping all available pages..."
          python scraper.py
        fi

    # Step 5: Verify that output file was created
    # - Sets output variable 'exists' to true/false
    # - Used by subsequent steps to conditionally run
    - name: Check if output file exists
      id: check_file
      run: |
        if [ -f ga_legislation.json ]; then
          echo "exists=true" >> $GITHUB_OUTPUT
          echo "JSON file created successfully"
          echo "File size: $(wc -c < ga_legislation.json) bytes"
        else
          echo "exists=false" >> $GITHUB_OUTPUT
          echo "Warning: JSON file was not created"
        fi

    # Step 6: Generate timestamp for artifact naming
    # - Used to create unique artifact names for each run
    # - Helps track when data was collected
    - name: Get current date
      id: date
      run: echo "date=$(date +'%Y-%m-%d_%H-%M-%S')" >> $GITHUB_OUTPUT

    # Step 7: Upload results as GitHub Artifact
    # - Only runs if output file was successfully created
    # - Artifacts are retained for 90 days
    # - Can be downloaded from the Actions tab in GitHub
    - name: Upload JSON as artifact
      if: steps.check_file.outputs.exists == 'true'
      uses: actions/upload-artifact@v6
      with:
        name: ga-legislation-${{ steps.date.outputs.date }}
        path: ga_legislation.json
        retention-days: 90

    # Step 8: Optional - Commit and push results to repository
    # Uncomment below to automatically commit scraped data to the repository
    # This is disabled by default to avoid cluttering commit history
    # Enable if you want automatic data updates in your repo
    # - name: Commit and push if changes
    #   if: steps.check_file.outputs.exists == 'true'
    #   run: |
    #     git config --local user.email "github-actions[bot]@users.noreply.github.com"
    #     git config --local user.name "github-actions[bot]"
    #     git add ga_legislation.json
    #     git diff --quiet && git diff --staged --quiet || \
    #       (git commit -m "Update legislation data - ${{ steps.date.outputs.date }}" && git push)
