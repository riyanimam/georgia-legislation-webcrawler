name: Scrape Georgia Legislation

# Trigger configuration
on:
  # Manual trigger - allows running the workflow from GitHub UI
  # Users can optionally specify max_pages parameter to limit scraping
  workflow_dispatch:
    inputs:
      max_pages:
        description: Maximum pages to scrape (leave empty for all pages)
        required: false
        default: ''
  # Scheduled trigger
  schedule:
  - cron: 0 4 * * *

jobs:
  scrape:
    runs-on: ubuntu-latest
    # 2-hour timeout for full session scrapes
    # Adjust if your scraping takes longer
    timeout-minutes: 120

    steps:
    # Step 1: Clone the repository to get the latest code
    - name: Checkout repository
      uses: actions/checkout@v6

    # Step 2: Set up Python 3.11 environment
    # Matches local development requirements
    - name: Set up Python
      uses: actions/setup-python@v6
      with:
        python-version: '3.11'

    # Step 3: Install Python dependencies and Playwright browser
    # - requests: HTTP library for making web requests
    # - beautifulsoup4: HTML parsing library
    # - playwright: Browser automation for JavaScript rendering
    # - aiohttp: Async HTTP client for concurrent requests
    # - chromium: Headless browser for rendering JavaScript-heavy pages
    - name: Install dependencies
      run: |
        pip install requests beautifulsoup4 playwright aiohttp
        playwright install chromium

    # Step 4: Set up Node.js for validation scripts
    - name: Set up Node.js
      uses: actions/setup-node@v6
      with:
        node-version: '20'

    # Step 5: Install Node.js validation tools
    - name: Install Node.js tools
      run: npm install

    # Step 6: Run the scraper
    # - Respectfully scrapes with configurable delays to respect server
    # - Uses environment variables to control request rate
    # - continue-on-error: true allows workflow to continue if server blocks IPs
    # - Outputs ga_legislation.json to repository root
    - name: Run scraper
      continue-on-error: true
      env:
        SCRAPER_DELAY: '2'
        SCRAPER_CONCURRENCY: '2'
      run: |
        if [ -n "${{ github.event.inputs.max_pages }}" ]; then
          echo "Scraping maximum ${{ github.event.inputs.max_pages }} pages..."
          echo "Using respectful settings: 2s delay, 2 concurrent requests"
          python backend/scraper.py ${{ github.event.inputs.max_pages }}
        else
          echo "Scraping all available pages..."
          echo "Using respectful settings: 2s delay, 2 concurrent requests"
          python backend/scraper.py
        fi

        # Verify output file location
        if [ -f ga_legislation.json ]; then
          echo "✓ Output file created at repository root"
        else
          echo "⚠ Output file not found at repository root"
          # Check if it was created elsewhere
          find . -name "ga_legislation.json" -type f
        fi

    # Step 7: Verify that output file was created
    # - Sets output variable 'exists' to true/false
    # - Used by subsequent steps to conditionally run
    - name: Check if output file exists
      id: check_file
      run: |
        if [ -f ga_legislation.json ]; then
          echo "exists=true" >> $GITHUB_OUTPUT
          echo "JSON file created successfully"
          echo "File size: $(wc -c < ga_legislation.json) bytes"
        else
          echo "exists=false" >> $GITHUB_OUTPUT
          echo "Warning: JSON file was not created"
        fi

    # Step 8: Validate JSON schema
    # - Validates structure and content against defined schema
    # - Checks required fields, data types, and formats
    # - Reports statistics on valid/invalid bills
    - name: Validate JSON schema
      if: steps.check_file.outputs.exists == 'true'
      run: node scripts/validate-schema.js ga_legislation.json

    # Step 10: Upload results as GitHub Artifact
    # - Only runs if output file was successfully created
    # - Artifacts are retained for 7 days
    # - Can be downloaded from the Actions tab in GitHub
    - name: Upload JSON as artifact
      if: steps.check_file.outputs.exists == 'true'
      uses: actions/upload-artifact@v6
      with:
        name: ga-legislation
        path: ga_legislation.json
        retention-days: 7

    # Step 11: Create GitHub App token for bypass branch protection
    # Uses the GitHub App credentials to get a token that can bypass
    # branch protection rules and create pull requests
    - name: Generate GitHub App token
      id: app-token
      uses: actions/create-github-app-token@v2
      with:
        app-id: ${{ secrets.APP_ID }}
        private-key: ${{ secrets.PRIVATE_KEY }}
      if: |
        steps.check_file.outputs.exists == 'true'

    # Step 12: Create pull request and auto-approve/merge
    # Creates a new branch for the data update, commits the changes,
    # pushes to remote, auto-approves, and sets up auto-merge.
    - name: Create pull request with updated data
      if: |
        steps.check_file.outputs.exists == 'true'
      env:
        GH_TOKEN: ${{ steps.app-token.outputs.token }}
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"

        # Create a feature branch
        BRANCH_NAME="update-legislation"
        git checkout -b "$BRANCH_NAME"

        # Add the generated file (force add since it's in .gitignore)
        git add -f ga_legislation.json

        # Check if there are changes to commit
        if ! git diff --cached --quiet; then
          git commit -m "chore: update legislation data"
          git push origin "$BRANCH_NAME"

          # Create pull request using GitHub CLI
          PR_URL=$(gh pr create \
            --base main \
            --head "$BRANCH_NAME" \
            --title "chore: update legislation data" \
            --body "Automated legislation data update from web scraper")

          # Extract PR number from URL
          PR_NUMBER=$(echo "$PR_URL" | grep -oP '/pull/\K[0-9]+')

          echo "PR #$PR_NUMBER created"
          echo "PR_NUMBER=$PR_NUMBER" >> $GITHUB_ENV
        else
          echo "No changes detected in ga_legislation.json"
        fi

    # Step 13: Auto-approve and merge the PR
    # Uses hmarr/auto-approve-action to approve the PR created by the bot,
    # then enables auto-merge with squash strategy
    - name: Auto-approve and merge PR
      if: env.PR_NUMBER != ''
      uses: hmarr/auto-approve-action@v4
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        pull-request-number: ${{ env.PR_NUMBER }}

    - name: Enable auto-merge
      if: env.PR_NUMBER != ''
      env:
        GH_TOKEN: ${{ steps.app-token.outputs.token }}
      run: |
        gh pr merge ${{ env.PR_NUMBER }} --auto --squash
        echo "PR #${{ env.PR_NUMBER }} approved and set to auto-merge"
